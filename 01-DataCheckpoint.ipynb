{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "For each group member please list how they contributed to this project using these terms:\n",
    "> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n",
    "\n",
    "- Muhammed Abdalla: Software, Writing - original draft, Writing - review & editing\n",
    "- Katherine Chui: Software, Visualization, Writing - review & editing \n",
    "- Iha Gadiya: Analysis, Methodology, Visualization, Writing – review & editing\n",
    "- Jeanne Lee: Background research, Conceptualization, Analysis, Methodology, Visualizatioin, Writing - review & editing \n",
    "- Jacob Ortiz: Analysis, Background research, Conceptualization, Data curation, Expiremental investigation, Methodology, Project administration, Software, Writing – original draft, Writing – review & editing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "- Dataset #1:\n",
    "  - Dataset Name: CO₂ emissions per capita\n",
    "  - Link to the dataset: https://ourworldindata.org/grapher/co-emissions-per-capita?tab=table \n",
    "  - Number of observations: After restricting to 2005–2023 and real countries, the dataset contains 4085 country–year observations (check 02-EDACheckpoint.ipynb for how this was done.)\n",
    "  - Number of variables: 4, which correspond to the 4 columns in this dataset.\n",
    "  - Description of the variables most relevant to this project: \n",
    "    - entity: This is the country name, used to identify each country and interpret results.\n",
    "    - code: This is a 3 letter country code which helps us reliably merge datasets across sources.\n",
    "    - year: This is year of observation which is used to align all datasets on a shared time frame (2005 - 2023).\n",
    "    - emissions_total_per_capita: Total CO2 emissions per person which will be primary measure of human activity contributing to climate change and serves as the key independent variable in our analysis.\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project: \n",
    "    - National averages hide within country inequality.\n",
    "    - Does not capture historical prior to dataset year coverage.\n",
    "    - Does not directly measure local environmental exposure.\n",
    "- Dataset #2:\n",
    "  - Dataset Name: Rate of deaths and missing persons due to natural disasters, 2005 to 2023\n",
    "  - Link to the dataset: https://ourworldindata.org/grapher/deaths-and-missing-persons-due-to-natural-disasters?tab=line \n",
    "  - Number of observations: After restricting to 2005–2023 and real countries, the dataset contains 1966 country–year observations (check 02-EDACheckpoint.ipynb for how this was done.)\n",
    "  - Number of variables: 4, which correspond to the 4 columns in this dataset.\n",
    "  - Description of the variables most relevant to this project: \n",
    "    - entity: This is the country name, used to identify each country and interpret results.\n",
    "    - code: This is a 3 letter country code which helps us reliably merge datasets across sources.\n",
    "    - year: This is year of observation which is used to align all datasets on a shared time frame (2005 - 2023).\n",
    "    - _13_1_1__vc_dsr_mtmp: Rate of deaths and missing persons due to natural disasters per 100,000 population, this is used as a proxy for climate related harm and vulnerability.\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project: \n",
    "    - Underreporting likely in lower income countries.\n",
    "    - Not all natural disasters are necessarily solely climate driven.\n",
    "    - Mortality does not capture non lethal impacts such as displacement and destruction.\n",
    "- Dataset #3: \n",
    "  - Dataset Name: GDP per capita\n",
    "  - Link to the dataset: https://ourworldindata.org/grapher/gdp-per-capita-worldbank?tab=table\n",
    "  - Number of observations: After restricting to 2005–2023 and real countries, the dataset contains 4005 country–year observations (check 02-EDACheckpoint.ipynb for how this was done.)\n",
    "  - Number of variables: 5, which correspond to the 5 columns in this dataset\n",
    "  - Description of the variables most relevant to this project: \n",
    "    - entity: This is the country name, used to identify each country and interpret results.\n",
    "    - code: This is a 3 letter country code which helps us reliably merge datasets across sources.\n",
    "    - year: This is year of observation which is used to align all datasets on a shared time frame (2005 - 2023).\n",
    "    - ny_gdp_pcap_pp_kd: GDP per capita in constant USD which serves as a proxy for national wealth and income level, which we use to group countries into low and high income categories.\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project: \n",
    "    - Does not measure income inequality within countries.\n",
    "    - 'Informal' economies not fully captured or those who may not be a part of the World bank.\n",
    "    - Wealth does not necessarily equal adaptive capacity in all cases.\n",
    "\n",
    "\n",
    "All three datasets are structured by country and year, which allows them to be merged into a single dataset using these shared columns. After aligning the datasets to a common time range (2005–2020) and removing missing values, each row in the combined dataset represents a single country in a specific year, with corresponding values for $CO_2$ emissions per capita, disaster impact, and GDP per capita. Countries will then be grouped into lower-income and higher-income categories based on GDP per capita to support comparative analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Download Progress:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading airline-safety.csv:   0%|          | 0.00/1.23k [00:00<?, ?B/s]\u001b[A\n",
      "Overall Download Progress:  50%|█████     | 1/2 [00:00<00:00,  8.74it/s]   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: airline-safety.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading bad-drivers.csv:   0%|          | 0.00/1.37k [00:00<?, ?B/s]\u001b[A\n",
      "Overall Download Progress: 100%|██████████| 2/2 [00:00<00:00,  8.02it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: bad-drivers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/airline-safety/airline-safety.csv', 'filename':'airline-safety.csv'},\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/bad-drivers/bad-drivers.csv', 'filename':'bad-drivers.csv'}\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1 CO_2 Emissions Per Capita \n",
    "\n",
    "This dataset tracks carbon dioxide emissions per person across countries from 2005 to 2023. CO_2 is the primary greenhouse gas of driving climate change, and measuring it on a per capita basis allows for fair comparison between countries of different popullation sizes. \n",
    "\n",
    "The key metric is \"emissions_total_per_capita\", meaured in tonnes per person. This represents the average amount of CO_2 emissions attributable to each indvidiaul ina country for a given year, calculated by dividing total national emissions by population. For context, the global average is approximately 4.7 to 5.0 tonnes per person. However, values vary dramatically when it comes to different economic states. Low-income nations may report below 0.1 tonnes while strong eceonomic areas typically range from 2 to 15 tonnes.\n",
    "\n",
    "Each row in the dataset corresponds to a single country for a specific year, with code providinga  three-letter abbreviation for each country to allow easier mergining with other datasets. Only countries with valid ISO codes are included as entities, while other regions or aggregates are excluded. Years are also restricted to 2005-2023 to ensure consistency across analyses. \n",
    "\n",
    "There are several considerations when using this dataset. First, national averages hide within country inequality. Emission can vary dramatically between urban and rural areas across income group within the same nation. Second, the data uses production based accounting, meaning emission are counted where good are produced rather than where they are consumed. This means a country importing manufactured goods appears cleaner whil the producing country carries the emissions burden. Third, the dataset exlcudes emissions from land use changes and international shipping are only included in global totals, not assinged to individual countries. Finally, data quality varies by country, as estimates rely on national statistics taht may be less reliable for nations with limited statistical infrastructure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Initial shape: (26509, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load dataset\n",
    "co2 = pd.read_csv(\"https://ourworldindata.org/grapher/co-emissions-per-capita.csv?v=1&csvType=full&useColumnShortNames=true\", \n",
    "                  storage_options = {'User-Agent': 'Our World In Data data fetch/1.0'},\n",
    "                  na_values =['', ' ', 'NA', 'N/A']\n",
    "                 )\n",
    "print(f\"Dataset loaded successfully!\") \n",
    "print(f\"Initial shape: {co2.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET SIZE\n",
      "----------------------------------------\n",
      "Total observations: 26,509 rows\n",
      "total variables: 5 columns\n",
      "unique countries/entities: 231\n",
      "Year range: 1750 to 2024\n",
      "Unique years: 230\n"
     ]
    }
   ],
   "source": [
    "## demonstrate size of dataset \n",
    "\n",
    "print(\"DATASET SIZE\")\n",
    "print(\"-\"*40) \n",
    "\n",
    "print(f\"Total observations: {co2.shape[0]:,} rows\")\n",
    "print(f\"total variables: {co2.shape[1]} columns\") \n",
    "print(f\"unique countries/entities: {co2['entity'].nunique():,}\")\n",
    "print(f\"Year range: {co2['year'].min()} to {co2['year'].max()}\")\n",
    "print(f\"Unique years: {co2['year'].nunique()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING DATA ANALYSIS\n",
      "----------------------------------------\n",
      "Missing values by column:\n",
      " entity: 0 missing\n",
      " code: 3,303 missing (12.5%)\n",
      " year: 0 missing\n",
      " emissions_total_per_capita: 0 missing\n",
      " is_outlier: 0 missing\n",
      "\n",
      "Entities without codes: 16\n",
      "Examples ['Africa', 'Asia', 'Asia (excl. China and India)', 'Europe', 'Europe (excl. EU-27)']\n"
     ]
    }
   ],
   "source": [
    "## missing data analysis \n",
    "print(\"MISSING DATA ANALYSIS\") \n",
    "print(\"-\"*40) \n",
    "\n",
    "missing = co2.isnull().sum() \n",
    "print(\"Missing values by column:\") \n",
    "for col in co2.columns: \n",
    "    print (f\" {col}: {missing[col]:,} missing ({missing[col]/len(co2)*100:.1f}%)\" if missing[col]>0 else f\" {col}: 0 missing\") \n",
    "\n",
    "no_code = co2[co2['code'].isna()] ['entity'] \n",
    "print(f\"\\nEntities without codes: {no_code.nunique()}\") \n",
    "print(\"Examples\", list(no_code.unique())[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6, DATA CLEANING\n",
      "----------------------------------------\n",
      "Outliers: 2114 (7.974650%)\n",
      "\n",
      "Top 10 highest emissions values:\n",
      "                   entity  year  emissions_total_per_capita\n",
      "Sint Maarten (Dutch part)  1954                   782.74340\n",
      "Sint Maarten (Dutch part)  1956                   741.27240\n",
      "Sint Maarten (Dutch part)  1951                   682.20276\n",
      "Sint Maarten (Dutch part)  1955                   610.66766\n",
      "Sint Maarten (Dutch part)  1957                   475.29102\n",
      "Sint Maarten (Dutch part)  1959                   471.24840\n",
      "Sint Maarten (Dutch part)  1950                   463.70790\n",
      "Sint Maarten (Dutch part)  1953                   408.73978\n",
      "Sint Maarten (Dutch part)  1958                   390.17386\n",
      "Sint Maarten (Dutch part)  1960                   376.49814\n",
      "/Processed data saved\n"
     ]
    }
   ],
   "source": [
    "## clean the data \n",
    "print(\"\\n6, DATA CLEANING\") \n",
    "print(\"-\"*40) \n",
    "\n",
    "co2_clean = co2[(co2['year'] >= 2025) & (co2['year'] <=2023) & (co2['code'].notna())].copy() \n",
    "\n",
    "## find and flag outliers \n",
    "q1, q3 = co2['emissions_total_per_capita'].quantile([0.25, 0.75]) \n",
    "co2['is_outlier'] = (co2['emissions_total_per_capita'] > q3 + 1.5*(q3-q1)).astype(int) \n",
    "print(f\"Outliers: {co2['is_outlier'].sum()} ({co2['is_outlier'].sum()/len(co2)*100:1f}%)\") \n",
    "\n",
    "print(\"\\nTop 10 highest emissions values:\") \n",
    "top_outliers = co2.nlargest(10, 'emissions_total_per_capita')[['entity', 'year', 'emissions_total_per_capita']]\n",
    "print(top_outliers.to_string(index=False)) \n",
    "\n",
    "#Save \n",
    "co2_clean.to_csv('data/02-processed/co2-emissions-processed.csv', index=False) \n",
    "print(\"/Processed data saved\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL SUMMARY\n",
      "----------------------------------------\n",
      "Rows: 4,085 | Countries: 215\n",
      "Years: 2005-2023\n",
      "Variables: entity, code, year, emissions_total_per_capita\n",
      "DATASET READY\n"
     ]
    }
   ],
   "source": [
    "## final summary \n",
    "print(\"FINAL SUMMARY\") \n",
    "print(\"-\"*40) \n",
    "print(f\"Rows: {len(co2_countries):,} | Countries: {co2_countries['entity'].nunique()}\") \n",
    "print(f\"Years: {co2_countries['year'].min()}-{co2_countries['year'].max()}\")\n",
    "print(f\"Variables: {', '.join(co2_countries.columns)}\") \n",
    "print(\"DATASET READY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #2 \n",
    "\n",
    "See instructions above for Dataset #1.  Feel free to keep adding as many more datasets as you need.  Put each new dataset in its own section just like these. \n",
    "\n",
    "Lastly if you do have multiple datasets, add another section where you demonstrate how you will join, align, cross-reference or whatever to combine data from the different datasets\n",
    "\n",
    "Please note that you can always keep adding more datasets in the future if these datasets you turn in for the checkpoint aren't sufficient.  The goal here is demonstrate that you can obtain and wrangle data.  You are not tied down to only use what you turn in right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Replace this with your timeline.  **PLEASE UPDATE your Timeline!** No battle plan survives contact with the enemy, so make sure we understand how your plans have changed.  Also if you have lost points on the previous checkpoint fix them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
